Какво е web crawler/spider/robot? Това е програма, която се използва за продължителното събиране (включва обновяване) на съдържание от Интернета, включва HTML документи, виедо и аудио съдържание, ПДФ документи и др. Web crawler-ите са често използвани за:
- Индексиране на страниците от search engine-и
- Архивиране на съфържание
- Web Mining или търсене на ново полезно съдържание
- Web Monitoring с цел контрол над патентовани ИС
Web crawler-ът работи общо казано като проверява различни страници, анализира и/или тегли съдържанието, намира референции към други страници и повтаря процеса.

Изискванията за нашият "паяк" са:
- Да събира по 1 милиярд страници на месец, тоест по 1млрд. / 30 дни / 24 часа / 3600 секунди = 400 страници в секунда.
- Тези страници трябва да бъдат записвани, тоест теглени и запазвани на диск в перод от 5 години, тоест ако предположим, че нужната ни памет за една страница е 500KB. 1млрд. * 500 килобайта * 12 месеца * 5 години = 30PB нужна памет.
- Паякът трябва да тегли страниците, както казах, но трябва да осигурява минимална дубликация и също така актуалност.
- За системата трябва да помислим над няколко сфери - Скалируемост, Politeness или как да не претоварваме сървърите, които crawl-ваме, как да се справяме с грешки и невалидни страници и как можем да разширим функциалността.

Така започваме с дизайна на системата. Тя ще е изградена от различни компоненти, които ще отговарят за отделни стадии.
1. URL Seed - Това е статичен компонент, по-скоро списък с начални точки. Паякът започва "traverse-ването" на уеба от тези страници. Би било удачно да имаме стратегия, било тя на база географска локации, различна тематика или друга.
2. URL Frontier - Това е един от най-важните компоненти на паяка. Това представлява една ФИФО опашка, която подава линкове, които трябва да бъдат проверени и обработени. Има няколко важни под компонента, които ще трябва да обсъдим на по-късен етап, но първо ни трябва цялостна идея за системата.
3. HTML Downloader - Този компонент получава от фронтиер-а линк и има за цел да го посети и да изтегли документа.
4. DNS Resolver - Използва се от downloader-a, който подава линкове към резолвъра и в замяна получава ИП-тата, от които може да получи ресурс. Понякога този момент може да се превърне в ботълнек, понеже look up времето може да стигне до 200мс, в такъв случай е добра идея да кешираме ИП-тата.
5. Content Parser - Този компонент има за цел да прегледа изтегленият компонент и да го валидира. Тук целта ни е отсеем невалидни документи, които биха предизвикали грешки.
6. Content Seen - Задължителен компонент за да сме сигурни, че отговаряме на изискването за минимална дубликация. Тук страниците се хешират и се сравняват с цел премахването на дублирани страници. Трябва да се запомни, че не проверяваме символ по символ страницата, а я хешираме. 
7. Content Storage - Тук записваме страниците. По-голямата част ще записваме на само диск, а популярните страници също се кешират в памет.
Тук приключва частта, която ни позволява да запазваме страниците, а в следващата част от компоненти ние продължаваме процеса чрез добавяне и менажиране на нови УРЛ-и.
8. Link Extractor - Търси линкове в HTML документа и ги заделя, като релативните УРЛ-и се конвертират в абсолютни. 
9. Link Filter - Филтрира УРЛ-и, към които нямам интерес, да кажем в случая - снимки, аудио, видео и т.н.
10. Link Seen - Този компонент се състои от структура/и от данни, която/които събират посетените линкове или тези, които в момента чакат реда си и проверява дали се опитваме да добавим линкове, които вече са видяни. Ако са, те биват премахвани.

Така тук приключва дизайнът на високо ниво, но още не сме отговорили на всички изисквания. Нека започнем с politeness. За да разберем какво се има предвид под "учтивост" трябва да знаем принципа на траверсване на страниците, по принцип можем да мислим за страниците като едно дърво, една страница води до други страници. За да траверснем това дърпо по принцип използваме ДФС или БФС, в случая ДФС не е подгодящ алгоритъм поради факта, че можем да стигнем до прекалено голяма дълбочина. Тоест използваме БФС, където страниците се разглеждат по реда на добавяне. Това води до проблем, които се състои в това, че в някакъв момент можем да стигнем до хиляди заявки в секунда към един сървър и така да го претоварим, а дори и да се сметне като ДОС атака. За да се справим с този проблем ние използваме подход, при който се лимитираме да теглим само по една страница в едно и също време. Това се случва чрез отделни опашки и отговрни worker threads или работници. Всеки работник си има опашка, за която отговаря, тази информация се държи в някакъв мап. Всяка опашка отговаря за страниците от един базов домейн. Втори проблем с БФС травеса е че нямам приоритизация, тоест ние бихме искали главните страници на сайтовете да се преглеждат преди случайните блог постове. И този проблем решаваме с Н на брой опашки, където страниците се разпределят помежду опашки според ранк-а си, честотата на обновление и трафика, който носят на сайта, като всяка опашка има приоритет над следващата.

Така има няколко неща, които трябва да имаме в предвид за всеки сайт. Винаги трябва да проверяваме а Robot Exclusion Protocol, това е един txt файл, който се предоставя на работите за да им се даде информация, за това кои страници могат да се теглят. Другото нещо са така наречениете Spider Trap-ове, които вкарват систамета в безкраен цикъл. Трябва да се справим и с data noise-а, тоест реклами, спам и други незначителни за нас елементи.

От към неща, които можем да направим за да подобрим performance са станартните неща. Дистрибутиране на системата, като пускаме няколко сървъра с паяци, всеки от който има определен брой работници. Тези сървръри би било удачно да са на различни географски локации и да отговарят за най-близките до тях сайтове. Данните могат да бъдат репликирани и шарднати и т.н.

Последното нещо е надграждането на паяка, понеже работим с модули, винаги можем да добавим нов модул. Да кажем за теглене на снимки.